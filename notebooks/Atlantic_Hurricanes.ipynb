{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "The data for this project is in the HURDAT2 format from NHC. This format contains rows of storm positions interspersed with header rows denoting which storm the subsequent position data corresponds to.\n",
    "\n",
    "Because of this, the raw data table has a few problems:\n",
    "- No column names are provided.\n",
    "- Columns contain a mix of data types.\n",
    "- Many rows are full of missing data for most columns.\n",
    "\n",
    "As a result, our first goal will be to convert the data into a more usable format. We begin by importing the raw data as is into a Pandas DataFrame, `atl`. In doing so, we also assign column names which correspond to the information in the storm position data rows. We will later separate the header rows into a new DataFrame and assign them their own column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>recordID</th>\n",
       "      <th>status</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>maxSustWind</th>\n",
       "      <th>minPressure</th>\n",
       "      <th>extNE34</th>\n",
       "      <th>extSE34</th>\n",
       "      <th>extSW34</th>\n",
       "      <th>extNW34</th>\n",
       "      <th>extNE50</th>\n",
       "      <th>extSE50</th>\n",
       "      <th>extSW50</th>\n",
       "      <th>extNW50</th>\n",
       "      <th>extNE64</th>\n",
       "      <th>extSE64</th>\n",
       "      <th>extSW64</th>\n",
       "      <th>extNW64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL011851</td>\n",
       "      <td>UNNAMED</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18510625</td>\n",
       "      <td>0000</td>\n",
       "      <td></td>\n",
       "      <td>HU</td>\n",
       "      <td>28.0N</td>\n",
       "      <td>94.8W</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18510625</td>\n",
       "      <td>0600</td>\n",
       "      <td></td>\n",
       "      <td>HU</td>\n",
       "      <td>28.0N</td>\n",
       "      <td>95.4W</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18510625</td>\n",
       "      <td>1200</td>\n",
       "      <td></td>\n",
       "      <td>HU</td>\n",
       "      <td>28.0N</td>\n",
       "      <td>96.0W</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18510625</td>\n",
       "      <td>1800</td>\n",
       "      <td></td>\n",
       "      <td>HU</td>\n",
       "      <td>28.1N</td>\n",
       "      <td>96.5W</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date                 time recordID status     lat     long  \\\n",
       "0  AL011851              UNNAMED       14    NaN     NaN      NaN   \n",
       "1  18510625                 0000              HU   28.0N    94.8W   \n",
       "2  18510625                 0600              HU   28.0N    95.4W   \n",
       "3  18510625                 1200              HU   28.0N    96.0W   \n",
       "4  18510625                 1800              HU   28.1N    96.5W   \n",
       "\n",
       "   maxSustWind  minPressure  extNE34  extSE34  extSW34  extNW34  extNE50  \\\n",
       "0          NaN          NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "1         80.0       -999.0   -999.0   -999.0   -999.0   -999.0   -999.0   \n",
       "2         80.0       -999.0   -999.0   -999.0   -999.0   -999.0   -999.0   \n",
       "3         80.0       -999.0   -999.0   -999.0   -999.0   -999.0   -999.0   \n",
       "4         80.0       -999.0   -999.0   -999.0   -999.0   -999.0   -999.0   \n",
       "\n",
       "   extSE50  extSW50  extNW50  extNE64  extSE64  extSW64  extNW64  \n",
       "0      NaN      NaN      NaN      NaN      NaN      NaN      NaN  \n",
       "1   -999.0   -999.0   -999.0   -999.0   -999.0   -999.0   -999.0  \n",
       "2   -999.0   -999.0   -999.0   -999.0   -999.0   -999.0   -999.0  \n",
       "3   -999.0   -999.0   -999.0   -999.0   -999.0   -999.0   -999.0  \n",
       "4   -999.0   -999.0   -999.0   -999.0   -999.0   -999.0   -999.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# create a list of column names\n",
    "\n",
    "header = ['date', 'time', 'recordID', 'status', 'lat', 'long', 'maxSustWind', 'minPressure', 'extNE34', 'extSE34', 'extSW34', 'extNW34', 'extNE50', 'extSE50', 'extSW50', 'extNW50', 'extNE64', 'extSE64', 'extSW64', 'extNW64']\n",
    "\n",
    "\n",
    "# import Best Track Data (HURDAT2) using out column names, and verify the new DataFrame.\n",
    "\n",
    "atl = pd.read_csv('atlantic.csv', names = header)\n",
    "atl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the NHC documentation for the HURDAT2 database, column names are as follows:\n",
    "\n",
    "- `date`: timecode for the position entry, format `YYYYMMDD`\n",
    "- `time`: timecode for the position entry, format `HHMM` in 24-Hr UTC\n",
    "- `recordID`: special designation for significant position entries. Can be empty or contain values:\n",
    " - **C**: closest approach to coast when not followed by landfall\n",
    " - **G**: genesis\n",
    " - **I**: intensity peak in both pressure and wind\n",
    " - **L**: landfall\n",
    " - **P**: minimum pressure\n",
    " - **R**: additional intensity detail during rapid changes\n",
    " - **S**: change of status\n",
    " - **T**: additional track/position detail\n",
    " - **W**: maximum wind speed\n",
    "- `status`: tropical depression, tropical storm, hurricane, extratropical cyclone, subtropical depression, subtropical storm, low pressure system, tropical wave, or disturbance\n",
    "- `lat`: latitude of center of storm\n",
    "- `long`: longitude of center of storm\n",
    "- `maxSustWind`: maximum sustained wind\n",
    "- `minPressure`: minimum central pressure\n",
    "- `extDDXX`: extent of `XX` nautical mile per hour (knots) winds in the `DD` cardinal direction quadrant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to separate the rows of `atl` into two new DataFrames, `storms` for header rows and `positions` for position data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53747 = 1894 + 51853\n"
     ]
    }
   ],
   "source": [
    "# first, create a new column to denote whether rows are header rows (True) or data rows (False)\n",
    "\n",
    "\n",
    "header = [] # list to be used as new column atl['header']\n",
    "\n",
    "\n",
    "for entry in atl['date']:\n",
    "    \n",
    "    if entry.find('AL') != -1: # all header columns, and only header columns, contain 'AL'\n",
    "        header.append(True)\n",
    "    else:\n",
    "        header.append(False)\n",
    "\n",
    "        \n",
    "atl['header'] = pd.Series(header) # add the list as a pandas series into a column of atl dataFrame\n",
    "\n",
    "\n",
    "# create dataframes of only storm names and only position data so we can edit the columns and dtypes\n",
    "\n",
    "storms = atl[atl['header'] == True].copy() # all header columns of atl copied into new dataframe storms\n",
    "positions = atl[atl['header'] == False].copy() # all data columns of atl copied into new dataframe positions\n",
    "\n",
    "\n",
    "# confirm all rows were sorted into one dataframe or the other\n",
    "\n",
    "print(atl.shape[0], \"=\", storms.shape[0], \"+\", positions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our DataFrames, `storms` and `positions`, each one needs a bit more preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the storms dataframe, we need to remove unnecessary columns, rename existing columns, create a new\n",
    "#     year column, and assign the correct dtypes to all columns\n",
    "\n",
    "\n",
    "# drop unnecessary columns, rename remaining columns, and clean up indices\n",
    "\n",
    "storms.drop(['status', 'lat', 'long', 'maxSustWind', 'minPressure', 'extNE34', 'extSE34', 'extSW34', 'extNW34', 'extNE50', 'extSE50', 'extSW50', 'extNW50', 'extNE64', 'extSE64', 'extSW64', 'extNW64', 'header'], axis = 1, inplace = True)\n",
    "storms.columns = ['stormID', 'name', 'numPositions']\n",
    "storms.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column names are as follows:\n",
    "#\n",
    "# 'stormID': an individual identifier for each storm in the form ALXXYYYY denoting the storm was the XXth storm\n",
    "#     of (A)t(L)antic Hurricane Season YYYY. Useful when storms in different years share the same name, and for\n",
    "#     unnamed storms.\n",
    "# 'name': name of storm.\n",
    "# 'numPositions': the number of position entries in positions dataFrame corresponding to this storm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the new columns to the correct dtypes\n",
    "\n",
    "stormYears = [] #create a new list to be used as a numeric years column\n",
    "\n",
    "for stormID in storms['stormID']:\n",
    "    stormYears.append(stormID[4:9]) # strip out the year from the stormID.\n",
    "                                    # note that this year may not necessarily correspond to the calendar dates\n",
    "                                    #      during which the storm existed, but rather the Hurricane Season to which\n",
    "                                    #      it belonged.\n",
    "    \n",
    "    \n",
    "storms['year'] = pd.Series(stormYears).astype('int') # assign new year column as integer dtype\n",
    "storms['numPositions'] = storms['numPositions'].astype('int') # reassign number of positions integer dtype\n",
    "storms['name'] = storms['name'].astype('str').str.strip() # reassign storm names string dtype and strip whitespace\n",
    "storms['stormID'] = storms['stormID'].astype('str').str.strip() # reassign stormID string dtype and strip whitespace\n",
    "\n",
    "\n",
    "# this completes work on the storms dataFrame. We can verify it now.\n",
    "\n",
    "storms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for the positions dataFrame we need to clean up the indices, reformat the latitude and longitude columns\n",
    "#     to make them usable by geopandas, create new columns for the storm name and stormId to make the \n",
    "#     dataframe searchable by these criteria, reform\n",
    "\n",
    "\n",
    "# clean up the indices\n",
    "\n",
    "positions.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can convert the latitude and longitude information into integers by removing the cardinal direction.\n",
    "# we can instead write XX.XW as -XX.X and XX.XE as XX.X.\n",
    "# we can also write XX.XN as XX.X and XX.XS as -XX.X.\n",
    "\n",
    "\n",
    "intLat = [] # create lists to be used as new series for latitude and longitude \n",
    "intLong = []\n",
    "        \n",
    "    \n",
    "for cardLat in positions['lat']:\n",
    "    if cardLat.find('N') != -1: # for latitudes of degrees north, strip the whitespace and N\n",
    "        intLat.append(cardLat.strip(\" N\"))\n",
    "    else: # for latitudes of degrees south, strip the whitespace and S, and add a negative to the front\n",
    "        intLat.append('-'+cardLat.strip(\" S\"))\n",
    "    \n",
    "for cardLong in positions['long']:\n",
    "    if cardLong.find('E') != -1: #for longitudes of degrees east, strip the whitespace and E\n",
    "        intLong.append(cardLong.strip(\" E\"))\n",
    "    else: # for longitudes of degrees west, strip the whitespace and W, and add a negative to the front\n",
    "        intLong.append('-'+cardLong.strip(\" W\"))\n",
    "\n",
    "        \n",
    "# replace the existing longitude and latitude columns with the new ones\n",
    "\n",
    "positions['lat'] = pd.Series(intLat).astype('float')\n",
    "positions['long'] = pd.Series(intLong).astype('float')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the number of position updates for each storm to create a column for the positions dataframe\n",
    "#containing the appropriate names\n",
    "\n",
    "\n",
    "stormNames = [] # create a list to be used as the names column for the positions dataFrame\n",
    "\n",
    "\n",
    "for i in range(len(storms)): # for each storm in the storms dataFrame...\n",
    "    for j in range(storms['numPositions'][i]): # for the number of rows indicated, add the storm name to the list\n",
    "        stormNames.append(storms['name'][i])\n",
    "        \n",
    "        \n",
    "#add the new list containing a name for every row of positions into positions dataFrame\n",
    "        \n",
    "positions['name'] = pd.Series(stormNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat the process for storm IDs\n",
    "\n",
    "\n",
    "stormIDs = []\n",
    "\n",
    "\n",
    "for i in range(len(storms)):\n",
    "    for j in range(storms['numPositions'][i]):\n",
    "        stormIDs.append(storms['stormID'][i])\n",
    "        \n",
    "        \n",
    "positions['stormID'] = pd.Series(stormIDs)\n",
    "positions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can filter the storm position data by the storm each data point is from... e.g.:\n",
    "\n",
    "def lookupID(stormName):\n",
    "    return storms[storms['name'] == stormName]\n",
    "\n",
    "lookupID('ISABEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now we can find the unique stormID by searching the name of a storm and use it to query the position table\n",
    "#     for data on a specific storm... e.g.:\n",
    "\n",
    "positions[positions['stormID'] == 'AL132003'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Plots based on StormID\n",
    "\n",
    "Now that we can look up stormIDs based on name, we can use the stormID to make other queries.\n",
    "\n",
    "Using GeoPandas, we can create a simple plot of the track of the storm we request using a stormID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTrack(toPlotID):\n",
    "    \n",
    "    # create a temporary dataframe containing position entries for the given stormID\n",
    "    \n",
    "    plotTrackDF = positions[positions['stormID'] == toPlotID].copy()\n",
    "                                                                    \n",
    "    #create a geodataframe from the x and y coordinates of the selected stormID\n",
    "    \n",
    "    plotTrackGDF = gpd.GeoDataFrame(plotTrackDF, geometry = gpd.points_from_xy(plotTrackDF['long'], plotTrackDF['lat']))\n",
    "    \n",
    "    #import the world map\n",
    "    world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "    \n",
    "    #plot the selected storm's coordinates over a background map of North America\n",
    "    plotTrackGDF.plot(ax = world[world['continent'] == 'North America'].plot(color = 'white', edgecolor = 'black'), color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTrack('AL132003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
